{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Langauge anaylsis\n",
    "- We need to identify the non-english publications to translate them to english\n",
    "- There is a attribute \"cfLang\" which is the language of the publication\n",
    "- need to look if every publication has a that attribute and not how many nan values are there\n",
    "- for detecting there are several ways\n",
    "1. using the langdetect library\n",
    "2. using the googletrans library\n",
    "3. using the textblob library\n",
    "4. directly entering it in deepL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "from langdetect import DetectorFactory, detect\n",
    "import openpyxl\n",
    "from Research_Scraper.Research_Scraper_Code import utils\n",
    "\n",
    "DetectorFactory.seed = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# load data\n",
    "publications = pd.read_csv('../wi_df_final_clean_keyword.csv', encoding='utf8')\n",
    "# load complete wi data\n",
    "wi_data = utils.get_wi_publication_data_df()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# print datatype of id column\n",
    "print(publications['id'].dtype)\n",
    "# print datatype of id column\n",
    "print(wi_data['id'].dtype)\n",
    "# set id column to string both\n",
    "\n",
    "publications['id'] = publications['id'].astype(str)\n",
    "wi_data['id'] = wi_data['id'].astype(str)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 4,
>>>>>>> 56da3a75ffae432f05e0c4c9e54788f28607f11b
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_1452\\4132290659.py:4: SettingWithCopyWarning: \n",
=======
      "/var/folders/4h/85bfh88x7tl03kbbf_kjd4hc0000gp/T/ipykernel_97891/4132290659.py:4: SettingWithCopyWarning: \n",
>>>>>>> 56da3a75ffae432f05e0c4c9e54788f28607f11b
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wi_pub_languages['cfLang_extracted'] = wi_pub_languages['cfLang'].apply(\n"
     ]
    }
   ],
   "source": [
    "# get langauage attribute\n",
    "wi_pub_languages = wi_data[['id', 'cfLang']]\n",
    "# cfLang contains a json object with the language of the publication, extract value from key cfName if not none\n",
    "wi_pub_languages['cfLang_extracted'] = wi_pub_languages['cfLang'].apply(\n",
    "    lambda x: x['cfName'] if x is not None else None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of publications with language: 3365\n",
      "Number of publications without language: 562\n",
      "Englisch            2625\n",
      "Deutsch              722\n",
      "Russisch               7\n",
      "Mehrere Sprachen       5\n",
      "Polnisch               2\n",
      "Portugiesisch          2\n",
      "Französisch            1\n",
      "Niederländisch         1\n",
      "Name: cfLang_extracted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# eda\n",
    "# check if every publication has a language\n",
    "print(f'Number of publications with language: {wi_pub_languages[\"cfLang_extracted\"].count()}')\n",
    "print(f'Number of publications without language: {wi_pub_languages[\"cfLang_extracted\"].isna().sum()}')\n",
    "\n",
    "# check what languages are there and how often\n",
    "print(wi_pub_languages['cfLang_extracted'].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# left join from publications to wi_pub_languages, keep cfLang_extracted from right only on id\n",
    "publications_joined = publications.merge(wi_pub_languages[['id', 'cfLang_extracted']], on='id', how='left')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# reduce columns for further analysis\n",
    "publications_with_lang = publications_joined[['id', 'cfTitle', 'cfAbstr', 'keywords', 'cfLang_extracted']]\n",
    "\n",
    "# filter out publications with no abstract and no keywords\n",
    "publications_with_lang = publications_with_lang[\n",
    "    ~publications_with_lang['cfAbstr'].isna() | ~publications_with_lang['keywords'].isna()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# map language to abbreviation\n",
    "publications_with_lang['lang'] = np.where(publications_with_lang['cfLang_extracted'] == 'Englisch', 'en',\n",
    "                                          np.where(publications_with_lang['cfLang_extracted'] == 'Deutsch', 'de',\n",
    "                                                   np.where(publications_with_lang['cfLang_extracted'].isna(), None,\n",
    "                                                            'other')))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Detecting language with langdetect"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'en'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_lang(text):\n",
    "    try:\n",
    "        # if type not str\n",
    "        if not isinstance(text, str):\n",
    "            return None\n",
    "        return detect(text)\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}, text : {text}')\n",
    "        return None\n",
    "\n",
    "detect_lang('Opening the Black Box of Classification: A Practical Comparison of Explainable Artificial Intelligence Techniques for Tabular Data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No features in text., text : 2\n",
      "Error: No features in text., text : -\n",
      "Error: No features in text., text : -\n",
      "Error: No features in text., text : -\n",
      "Error: No features in text., text : -\n",
      "Error: No features in text., text : -\n",
      "Error: No features in text., text : -\n"
     ]
    }
   ],
   "source": [
    "# apply langdetect to title, keywords and abstract\n",
    "publications_with_lang['langdetect_title'] = publications_with_lang['cfTitle'].apply(detect_lang)\n",
    "publications_with_lang['langdetect_keywords'] = publications_with_lang['keywords'].apply(detect_lang)\n",
    "publications_with_lang['langdetect_abstract'] = publications_with_lang['cfAbstr'].apply(detect_lang)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Evaluate langdetect by comparing with cfLang_extracted\n",
    "# our goal is to find out how many publications are falsely classified as english\n",
    "\n",
    "# check if langdetect_title is equal to lang\n",
    "title_counts  = publications_with_lang.groupby(['lang', 'langdetect_title']).size().reset_index(name='counts')\n",
    "title_counts = title_counts.sort_values(by='counts', ascending=False)\n",
    "\n",
    "# check if langdetect_keywords is equal to lang\n",
    "keywords_counts  = publications_with_lang.groupby(['lang', 'langdetect_title']).size().reset_index(name='counts')\n",
    "keywords_counts = keywords_counts.sort_values(by='counts', ascending=False)\n",
    "\n",
    "# check if langdetect_abstract is equal to lang\n",
    "abstract_counts  = publications_with_lang.groupby(['lang', 'langdetect_title']).size().reset_index(name='counts')\n",
    "abstract_counts = abstract_counts.sort_values(by='counts', ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create labelling file for Quang"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of publications with missing language: 134\n",
      "Number of publications with None language: 134\n"
     ]
    }
   ],
   "source": [
    "publications_labelling_df = publications_with_lang\n",
    "# if lang is nan set missing to true\n",
    "publications_labelling_df['missing'] = np.where(publications_labelling_df['lang'].isna(), True, False)\n",
    "\n",
    "# drop cflang_extracted\n",
    "publications_labelling_df = publications_labelling_df.drop(columns=['cfLang_extracted'])\n",
    "\n",
    "# count how often missing is true\n",
    "print(f'Number of publications with missing language: {publications_labelling_df[\"missing\"].sum()}')\n",
    "\n",
    "# count None in lang\n",
    "print(f'Number of publications with None language: {publications_labelling_df[\"lang\"].isna().sum()}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "# sort by missing descending, langdetect_title ascending\n",
    "publications_labelling_df = publications_labelling_df.sort_values(by=['missing', 'langdetect_title'], ascending=[False, True])\n",
    "\n",
    "# create empty columns for labelling\n",
    "# label_title_is_englisch\n",
    "publications_labelling_df['label_title_is_englisch'] = None\n",
    "# label_keywords_is_englisch\n",
    "publications_labelling_df['label_keywords_is_englisch'] = None\n",
    "# label_abstract_is_englisch\n",
    "publications_labelling_df['label_abstract_is_englisch'] = None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# export as excel\n",
    "#publications_labelling_df.to_excel('publications_labelling.xlsx', index=False)\n",
    "\n",
    "# export as csv\n",
    "#publications_labelling_df.to_csv('publications_labelling.csv', index=False)\n",
    "\n",
    "# read excel file as df\n",
    "publications_labelling_df_excel = pd.read_excel('publications_labelling.xlsx')\n",
    "\n",
    "# check if equal\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create file for NLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'cfTitle', 'cfUri', 'keywords', 'doi', 'srcAuthors', 'authors',\n",
      "       'cfAbstr', 'publYear', 'eid', 'data_source', 'log', 'result_flag',\n",
      "       'error', 'error_doi', 'keyword_clean', 'cfLang_extracted', 'lang'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "publications_nlp = publications_joined\n",
    "\n",
    "# show columns\n",
    "print(publications_nlp.columns)\n",
    "\n",
    "publications_nlp['lang'] = np.where(publications_nlp['cfLang_extracted'] == 'Englisch', 'en',\n",
    "                                          np.where(publications_nlp['cfLang_extracted'] == 'Deutsch', 'de',\n",
    "                                                   np.where(publications_nlp['cfLang_extracted'].isna(), None,\n",
    "                                                            'other')))\n",
    "\n",
    "# select columns\n",
    "publications_nlp = publications_nlp[['id', 'cfTitle', 'cfAbstr', 'keywords','keyword_clean', 'lang']]\n",
    "\n",
    "# save as csv\n",
    "publications_nlp.to_csv('publications_nlp.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## analyse labelled data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# read labelled data\n",
    "publications_labelled = pd.read_excel(r'publications_labelling_quang_done.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of publications with missing language: 134\n",
      "Number of publications with language: 2121\n"
     ]
    }
   ],
   "source": [
    "# count how often no lang\n",
    "print(f'Number of publications with missing language: {publications_labelled[\"missing\"].sum()}')\n",
    "\n",
    "# coutn how often lang is given\n",
    "print(f'Number of publications with language: {publications_labelled[\"lang\"].notna().sum()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of publications without lang: (134, 12)\n",
      "Number of different langdetect_title values: 4\n",
      "en    124\n",
      "de      8\n",
      "it      1\n",
      "no      1\n",
      "Name: langdetect_title, dtype: int64\n",
      "\n",
      "Sum of label_title_is_englisch: 125.0\n",
      "\n",
      "Number of different langdetect_keywords values: 5\n",
      "en    102\n",
      "de      4\n",
      "it      1\n",
      "ro      1\n",
      "fr      1\n",
      "Name: langdetect_keywords, dtype: int64\n",
      "\n",
      "Sum of label_keywords_is_englisch: 107.0\n",
      "\n",
      "Number of different langdetect_abstract values: 2\n",
      "en    124\n",
      "de      1\n",
      "Name: langdetect_abstract, dtype: int64\n",
      "\n",
      "Sum of label_abstract_is_englisch: 124.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter rows without lang\n",
    "pub_without_lang = publications_labelled[publications_labelled['lang'].isna()]\n",
    "print(f'Shape of publications without lang: {pub_without_lang.shape}')\n",
    "\n",
    "# count different langdetect_title values\n",
    "print(f'Number of different langdetect_title values: {pub_without_lang[\"langdetect_title\"].nunique()}')\n",
    "# show overview with counts\n",
    "print(pub_without_lang['langdetect_title'].value_counts())\n",
    "\n",
    "# sum of the label_title_is_englisch column\n",
    "print(f'\\nSum of label_title_is_englisch: {pub_without_lang[\"label_title_is_englisch\"].sum()}\\n')\n",
    "\n",
    "\n",
    "# count different langdetect_keywords values\n",
    "print(f'Number of different langdetect_keywords values: {pub_without_lang[\"langdetect_keywords\"].nunique()}')\n",
    "# show overview with counts\n",
    "print(pub_without_lang['langdetect_keywords'].value_counts())\n",
    "# sum uof the label_keywords_is_englisch column\n",
    "print(f'\\nSum of label_keywords_is_englisch: {pub_without_lang[\"label_keywords_is_englisch\"].sum()}\\n')\n",
    "\n",
    "# count different langdetect_abstract values\n",
    "print(f'Number of different langdetect_abstract values: {pub_without_lang[\"langdetect_abstract\"].nunique()}')\n",
    "# show overview with counts\n",
    "print(pub_without_lang[\"langdetect_abstract\"].value_counts())\n",
    "# sum of the label_abstract_is_englisch column\n",
    "print(f'\\nSum of label_abstract_is_englisch: {pub_without_lang[\"label_abstract_is_englisch\"].sum()}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accurary of langdetect is good but the amount of unflagged languages in cris is rather low (134 / 2255 files with abstract andor keywords). So we will use the deepl-detect method for publications that do not have a lang or are labelled as german by cris."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## count chars for translation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de       722\n",
      "other     18\n",
      "Name: lang, dtype: int64\n",
      "Number of rows with lang none: 562\n"
     ]
    }
   ],
   "source": [
    "# filter publications_nlp with lang not en\n",
    "publications_nlp_not_en = publications_nlp[publications_nlp['lang'] != 'en']\n",
    "\n",
    "# overview of the counts of lang\n",
    "print(publications_nlp_not_en['lang'].value_counts())\n",
    "\n",
    "# count how many rows with lang none\n",
    "print(f'Number of rows with lang none: {publications_nlp_not_en[\"lang\"].isna().sum()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars in title column: 100513\n",
      "Number of chars in abstract column: 292452.0\n",
      "Number of chars in keywords column: 43563.0\n",
      "Total number of chars: 436528.0\n"
     ]
    }
   ],
   "source": [
    "# count chars for title column\n",
    "title_chars  = publications_nlp_not_en['cfTitle'].str.len().sum()\n",
    "print(f'Number of chars in title column: {title_chars}')\n",
    "\n",
    "# count chars for abstract column\n",
    "abstract_chars = publications_nlp_not_en['cfAbstr'].str.len().sum()\n",
    "print(f'Number of chars in abstract column: {abstract_chars}')\n",
    "\n",
    "# count chars for keywords column\n",
    "keywords_chars = publications_nlp_not_en['keyword_clean'].str.len().sum()\n",
    "print(f'Number of chars in keywords column: {keywords_chars}')\n",
    "\n",
    "total_chars = title_chars + abstract_chars + keywords_chars\n",
    "print(f'Total number of chars: {total_chars}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that the amount of chars is quite low and it is enough for the free tier / lowest tier plan of deepl. Thus we will not use advances lang detect methods"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look how many rows of the clean data are not englisch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(3927, 6)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications_nlp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of publications_nlp_filtered: (2748, 6)\n",
      "en       2155\n",
      "de        358\n",
      "other      11\n",
      "Name: lang, dtype: int64\n",
      "Number of rows with lang none: 224\n"
     ]
    }
   ],
   "source": [
    "# fiilter rows out where keyword and abstract is both none\n",
    "publications_nlp_filtered = publications_nlp[publications_nlp['cfAbstr'].notna() | publications_nlp['keyword_clean'].notna()]\n",
    "print(f'Shape of publications_nlp_filtered: {publications_nlp_filtered.shape}')\n",
    "\n",
    "\n",
    "# count lang values\n",
    "print(publications_nlp_filtered['lang'].value_counts())\n",
    "print(f'Number of rows with lang none: {publications_nlp_filtered[\"lang\"].isna().sum()}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
